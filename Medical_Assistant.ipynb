{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement"
      ],
      "metadata": {
        "id": "Kd2Yq2DtziCc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Business Context\n",
        "\n",
        "The healthcare professionals facing increasing challenges in managing vast volumes of medical data while delivering accurate and timely diagnoses. The need for quick access to comprehensive, reliable, and up-to-date medical knowledge is critical for improving patient outcomes and ensuring informed decision-making in a fast-paced environment.\n",
        "\n",
        "Healthcare professionals often encounter information overload, struggling to sift through extensive research and data to create accurate diagnoses and treatment plans. This challenge is amplified by the need for efficiency, particularly in emergencies, where time-sensitive decisions are vital. Furthermore, access to trusted, current medical information from renowned manuals and research papers is essential for maintaining high standards of care.\n",
        "\n",
        "To address these challenges, healthcare centers can focus on integrating systems that streamline access to medical knowledge, provide tools to support quick decision-making, and enhance efficiency. Leveraging centralized knowledge platforms and ensuring healthcare providers have continuous access to reliable resources can significantly improve patient care and operational effectiveness.\n",
        "\n",
        "\n",
        "\n",
        "Objective\n",
        "\n",
        "As an AI specialist, your task is to develop a RAG-based AI solution using renowned medical manuals to address healthcare challenges.\n",
        "\n",
        "The objective is to understand issues like information overload and apply AI techniques to streamline decision-making.\n",
        "\n",
        "To analyze its impact on diagnostics and patient outcomes, evaluate its potential to standardize care practices, and create a functional prototype demonstrating its feasibility and effectiveness.\n",
        "\n",
        "Data Description\n",
        "\n",
        "The Merck Manuals are medical references published by the American pharmaceutical company Merck & Co., that cover a wide range of medical topics, including disorders, tests, diagnoses, and drugs. The manuals have been published since 1899, when Merck & Co. was still a subsidiary of the German company Merck.\n",
        "\n",
        "The manual is provided as a PDF with over 4,000 pages divided into 23 sections.\n",
        "\n"
      ],
      "metadata": {
        "id": "e7FOz0gRzSrm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARMRGxLXVZMM"
      },
      "source": [
        "# Installing and Importing Necessary Libraries and Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q31zwMNOfg7V"
      },
      "outputs": [],
      "source": [
        "# Installation for GPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is being used\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q\n",
        "\n",
        "\n",
        "# Installation for CPU llama-cpp-python\n",
        "# uncomment and run the following code in case GPU is not being used\n",
        "#!CMAKE_ARGS=\"-DLLAMA_CUBLAS=off\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.85 --force-reinstall --no-cache-dir -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-HAjNX5-AgZL"
      },
      "outputs": [],
      "source": [
        "# For installing the libraries & downloading models from HF Hub\n",
        "\n",
        "!pip install -q \\\n",
        " langchain \\\n",
        " langchain-community \\\n",
        " chromadb \\\n",
        " sentence-transformers \\\n",
        " pymupdf \\\n",
        " tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kzeypNxUfp5D"
      },
      "outputs": [],
      "source": [
        "#Libraries for processing dataframes,text\n",
        "import json,os\n",
        "import tiktoken\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "\n",
        "#Libraries for downloading and loading the llm\n",
        "from huggingface_hub import hf_hub_download\n",
        "from llama_cpp import Llama\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Downloading and Loading the model"
      ],
      "metadata": {
        "id": "sTw99GgnNVy3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7PrrBuQcl0al"
      },
      "outputs": [],
      "source": [
        "# Downloading and Loading the model\n",
        "model_name_or_path = \"TheBloke/Mistral-7B-Instruct-v0.2-GGUF\"\n",
        "model_basename = \"mistral-7b-instruct-v0.2.Q6_K.gguf\"\n",
        "model_path = hf_hub_download(\n",
        "    repo_id=model_name_or_path,\n",
        "    filename=model_basename\n",
        ")\n",
        "# uncomment the below snippet of code if the runtime is connected to GPU.\n",
        "llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_ctx=2048,\n",
        "    n_gpu_layers=32,\n",
        "    n_batch=128\n",
        ")\n",
        "\n",
        "# uncomment the below snippet of code if the runtime is connected to CPU only.\n",
        "# llm = Llama(\n",
        "#    model_path=model_path,\n",
        "#    n_ctx=1024,\n",
        "#    n_cores=-2\n",
        "# )\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question & Answering using LLM"
      ],
      "metadata": {
        "id": "uVKmYXR_NxC_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7CMf63RTDuN"
      },
      "outputs": [],
      "source": [
        "#Response\n",
        "def response(query,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    model_output = llm(\n",
        "      prompt=query,\n",
        "      max_tokens=max_tokens,\n",
        "      temperature=temperature,\n",
        "      top_p=top_p,\n",
        "      top_k=top_k\n",
        "    )\n",
        "\n",
        "    return model_output['choices'][0]['text']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question & Answering using LLM\n"
      ],
      "metadata": {
        "id": "3DFD0Tr8N5AG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 1: What is the protocol for managing sepsis in a critical care unit?\n",
        "llm(\"What is the protocol for managing sepsis in a critical care unit?\")\n"
      ],
      "metadata": {
        "id": "5AvUWyjttROd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 2: What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n",
        "llm(\"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\")\n"
      ],
      "metadata": {
        "id": "vFYl_qBGtQgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n",
        "llm(\"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\")\n"
      ],
      "metadata": {
        "id": "o5fVieUttNzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n",
        "llm(\"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\")\n",
        "\n"
      ],
      "metadata": {
        "id": "8af3xzV-tNwI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n",
        "llm(\"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\")"
      ],
      "metadata": {
        "id": "_7k4o_NrtNtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations for raw llm answers:\n",
        "\n",
        "The model has generated a clinically grounded answers.The responses were incomplete, it was cut off mid length due to token length. The temperature is 0 so the answer is focused . The top p value is high so it has creative component also. So the answer is balance of creativity and relevance.  No hallucinations were seen. Some repetition was seen  in the answer.We can alter parameters and the prompt to get refined response.\n",
        "Answer 1 explains symptoms and management of symptoms briefly. Answer 2 gives detail of symptoms of appendicitis. The answer 3 gives information on causes of hair loss. The answer 4 gives management of patient suffering brain injury. The answer 5 includes assessment and precaution patient who has fractured leg."
      ],
      "metadata": {
        "id": "05vsCTTAEnpk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question Answering using LLM with Prompt Engineering"
      ],
      "metadata": {
        "id": "SJEuNPyjOS5c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will be using 5 different prompting techniques with llm parameters. The temperature controls randomness of response; higher temperature gives creative/random and lower temperature gives more focused output.\n",
        "The top p value it limits sampling to top tokens whose cumulative probability adds up to p higher values gives diverse and lower values gives more focused response.The repeat penalty it penalizes repeated tokens. The higher values have less repetition and lower values more likely to repeat words.The max token helps in adjusting the length of an answer. The higher values give detailed response and lower values concise response."
      ],
      "metadata": {
        "id": "P0VMZjHTbKqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 1:\n",
        "\n",
        "What is the protocol for managing sepsis in a critical care unit?\n"
      ],
      "metadata": {
        "id": "nX-f-EwYhQo4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"\"\"Example 1:\n",
        "Q: How is diabetic ketoacidosis managed in a hospital setting?\n",
        "A: Management includes fluid resuscitation with isotonic saline, insulin therapy to reduce blood glucose, and close monitoring of electrolytes, especially potassium. Regular blood gas analysis and continuous cardiac monitoring are essential.\n",
        "\n",
        "Example 2:\n",
        "Q: What is the treatment protocol for acute myocardial infarction in an emergency unit?\n",
        "A: The protocol includes administration of oxygen, aspirin, nitroglycerin, morphine (as needed), and beta-blockers. Rapid ECG, troponin testing, and preparation for reperfusion therapy such as PCI or thrombolytics are critical.\n",
        "\n",
        "Example 3:\n",
        "Q: What is the protocol for managing sepsis in a critical care unit?\n",
        "A: The management of sepsis includes \"\"\"\n",
        "\n",
        "# Generate response\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=300,\n",
        "    temperature=0,\n",
        "    top_p=0,\n",
        "    repeat_penalty=1,\n",
        "    stop=['Example 4:']\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(output['choices'][0]['text'].strip())\n"
      ],
      "metadata": {
        "id": "7vtCZ1IBbPsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this prompting technique we are using few short prompting, by providing a few task examples to guide the model's output. The top p value and temperature value is set at 0 so that answer is focused and concise. The repeat penalty is high so there are no repeating phrases. The max token helps in increasing length of answer.We can adjust top p, temperature, max token values to get diverse and detailed answers."
      ],
      "metadata": {
        "id": "7z1oijMue69t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 2:\n",
        "\n",
        "What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?"
      ],
      "metadata": {
        "id": "WzFCHg1PpgmG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"Let’s analyze this step by step:\n",
        "Step 1: Identify the clinical symptoms of appendicitis.\n",
        "Step 2: Explain the progression and risks of untreated appendicitis.\n",
        "Step 3: Determine when medical treatment may be used.\n",
        "Step 4: Describe when surgery is necessary and what procedure is standard.\n",
        "Step 5: Compare types of surgery and recovery outcomes.\"\"\"\n",
        "\n",
        "\n",
        "# Generate response\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.3,\n",
        "    top_p=0.5,\n",
        "    repeat_penalty=0.8,\n",
        "\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(output['choices'][0]['text'].strip())\n"
      ],
      "metadata": {
        "id": "bgrujIbpM1HS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are using chain of thought prompting, in this prompting we encourage model to use step-by-step reasoning. The temperature and top p value are slightly increased to provide consistent and contextually appropriate response. The repeat penalty is low so lot of repetition of phrases can be seen."
      ],
      "metadata": {
        "id": "g6-Kr58mk-kR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 3:\n",
        "\n",
        "What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?"
      ],
      "metadata": {
        "id": "pEM10uqBjmAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"You are a dermatologist providing a structured answer to a medical student.\n",
        "\n",
        "Please write a clinically accurate and structured response to:\n",
        "\n",
        "“What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?”\n",
        "\n",
        "Organize your answer as follows:\n",
        "1. Clinical Description & diagnosis\n",
        "2. Differential diagnosis\n",
        "3. Treatment Plan (based on cause)\n",
        "\n",
        "Keep the clarity of language and educational for students\"\"\"\n",
        "\n",
        "\n",
        "# Generate response\n",
        "output = llm(\n",
        "    prompt,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        "    repeat_penalty=1,\n",
        "    max_tokens=600\n",
        "\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(output['choices'][0]['text'].strip())"
      ],
      "metadata": {
        "id": "V1a49XdztnFR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this meta prompting technique it focuses on the structural and syntactical aspects of problems, prioritizing the general format and pattern over specific content details. The temperature and p value is raised to get diverse response. The repeat penalty is high so no reiteration of phrases can be seen.This combination has to be checked for possiblity of hallucination."
      ],
      "metadata": {
        "id": "d3aseOfqt35k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 4:\n",
        "\n",
        "What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?"
      ],
      "metadata": {
        "id": "AlmU1nI5laYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"Your are neuro-rehablitation therapist assessing patient suffering brain injury.\n",
        "Think: What types of brain injuries can lead to temporary or permanent impairment?\n",
        "Act: Describe how to assess the patient using imaging  and neurological scoring.\n",
        "Think: What interventions are needed for acute stabilization and prevention of secondary brain injury?\n",
        "Act: Outline specific treatments such as surgical decompression, ICP monitoring, or pharmacological neuroprotection.\n",
        "Think: What should be considered for rehabilitation and recovery?\n",
        "Act: Detail cognitive, physical, and psychosocial therapy approaches.\n",
        "Conclude by summarizing a multidisciplinary treatment plan.\"\"\"\n",
        "\n",
        "# Generate response\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0,\n",
        "    top_p=0,\n",
        "    repeat_penalty=1.2\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(output['choices'][0]['text'].strip())\n"
      ],
      "metadata": {
        "id": "5u9ha1yl3HWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this ReAct prompting, prompts guide the model to reason through a problem first, then decide which actions are necessary to reach the best solution. The top p and temperature is 0 to get to give steady and meaningful response. Here the repeat penalty is high so there is no repetition of phrases."
      ],
      "metadata": {
        "id": "0ajDheZHt8CQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Query 5:\n",
        "\n",
        "What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n"
      ],
      "metadata": {
        "id": "qn2GP2enmXq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt= \"\"\"Using your general knowledge of orthopedics, emergency response, and rehabilitation medicine, explain what a person should do if they fracture their leg during a hiking trip.\n",
        "Include considerations for:\n",
        "On-site first aid\n",
        "Transport and evacuation\n",
        "Hospital-based treatment options for management of pain and inflammation management\n",
        "Rehabilitation and recovery milestones\n",
        "Provide a clear, logical explanation grounded in clinical and survival principles.\"\"\"\n",
        "\n",
        "# Generate response\n",
        "output = llm(\n",
        "    prompt,\n",
        "    max_tokens=600,\n",
        "    temperature=0.9,\n",
        "    top_p=0.95,\n",
        "    repeat_penalty=1\n",
        ")\n",
        "\n",
        "# Print result\n",
        "print(output['choices'][0]['text'].strip())\n"
      ],
      "metadata": {
        "id": "BfhPGYbHZNbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this the generate knowledge type, the model generates factual knowledge before answering a question. Here temperature and top p value is raised to get a creative response. The repeat penalty is high so no reiteration of phrases can be seen. This combination has high risk of hallucination especially with medical information avoid using this combination.\n",
        "\n"
      ],
      "metadata": {
        "id": "vB1vv3lOu8D6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "\n",
        "If all llm parameters temperature, top p and repeat penalty are set 0 no response is seen.So the combinations with very high temperature has possiblity of deviation from the topic and also increases risk of hallucination. The high top p value allows for creativity. If repeat penalty is set high less repetition of phrases can be seen in the response."
      ],
      "metadata": {
        "id": "URNAimgKZP_N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Question and answering with RAG"
      ],
      "metadata": {
        "id": "jl5Xrlm0ccSN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation for RAG"
      ],
      "metadata": {
        "id": "w1MoxSwfKl3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLphx2opBAGH"
      },
      "outputs": [],
      "source": [
        "#Mount on google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/MyDrive/Colab Notebooks/data/medical_diagnosis_manual.pdf'"
      ],
      "metadata": {
        "id": "pPDu9OZe3y7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UYjTSeWgFEvR"
      },
      "outputs": [],
      "source": [
        "#Loading the data\n",
        "from langchain.document_loaders import PyMuPDFLoader\n",
        "\n",
        "pdf_loader = PyMuPDFLoader(filepath)\n",
        "\n",
        "medical_diagnosis_manual = pdf_loader.load()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-WwA8aYQZli"
      },
      "source": [
        "#Data Overview"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzDAPWYnPyk0"
      },
      "outputs": [],
      "source": [
        "#Checking the first 5 pages\n",
        "for i in range(5):\n",
        "    print(f\"Page Number : {i+1}\",end=\"\\n\")\n",
        "    print(medical_diagnosis_manual[i].page_content,end=\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IHm3cxl1U977"
      },
      "outputs": [],
      "source": [
        "#checking the number of pages\n",
        "len(medical_diagnosis_manual)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are 4114 pages."
      ],
      "metadata": {
        "id": "DAxFJI-WsMCK"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_RKmhhZyXLg6"
      },
      "source": [
        "# Data Chunking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40_UsEZoXJBg"
      },
      "outputs": [],
      "source": [
        "#data chunking\n",
        "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
        "    encoding_name='cl100k_base',\n",
        "    chunk_size=256,\n",
        "    chunk_overlap= 50\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RGgyC7w2Xil8"
      },
      "outputs": [],
      "source": [
        "# length of document checks\n",
        "document_chunks = pdf_loader.load_and_split(text_splitter)\n",
        "len(document_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHjt6jD1Xnmo"
      },
      "outputs": [],
      "source": [
        "#Checking the chunking\n",
        "document_chunks[0].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UvC-e4aXaz3m"
      },
      "outputs": [],
      "source": [
        "document_chunks[-2].page_content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OcnkaMNeayZy"
      },
      "outputs": [],
      "source": [
        "document_chunks[-1].page_content"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embedding Model"
      ],
      "metadata": {
        "id": "a98E33PZh_vd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#install sentence transformer\n",
        "!pip install sentence-transformers"
      ],
      "metadata": {
        "id": "xdXogVjGzd2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Libraries for Loading Data, Chunking, Embedding, and Vector Databases\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.document_loaders import PyMuPDFLoader\n",
        "from langchain_community.embeddings.sentence_transformer import SentenceTransformerEmbeddings\n",
        "from langchain_community.vectorstores import Chroma"
      ],
      "metadata": {
        "id": "2dMhlyyyzp2d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A1SNPwmWdUG6"
      },
      "outputs": [],
      "source": [
        "embedding_model = SentenceTransformerEmbeddings(model_name='thenlper/gte-large')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking embedding model\n",
        "embedding_1 = embedding_model.embed_query(document_chunks[0].page_content)\n",
        "embedding_2 = embedding_model.embed_query(document_chunks[1].page_content)\n",
        "print(\"Dimension of the embedding vector \",len(embedding_1))\n",
        "len(embedding_1)==len(embedding_2)"
      ],
      "metadata": {
        "id": "7IrA4yjust40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector database"
      ],
      "metadata": {
        "id": "c8FnGGNx1y4-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pVuul_WcenSU"
      },
      "outputs": [],
      "source": [
        "# Vector Database\n",
        "import os\n",
        "out_dir = '/content/drive/My Drive/medical_db'\n",
        "\n",
        "if not os.path.exists(out_dir):\n",
        "  os.makedirs(out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-qewuHMyhxyn"
      },
      "outputs": [],
      "source": [
        "vectorstore = Chroma.from_documents(\n",
        "    document_chunks,\n",
        "    embedding_model,\n",
        "    persist_directory=out_dir)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.persist()"
      ],
      "metadata": {
        "id": "uXsNIBDg1Wbb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#after restart run this\n",
        "vectorstore = Chroma(persist_directory=out_dir,embedding_function=embedding_model)"
      ],
      "metadata": {
        "id": "e1bdQn1DSofs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorstore.embeddings"
      ],
      "metadata": {
        "id": "opZ5ycHfSkVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking vector database\n",
        "vectorstore.similarity_search(\"Hair loss\",k=3)"
      ],
      "metadata": {
        "id": "1j3UGpA-SheU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Retriever"
      ],
      "metadata": {
        "id": "LC7ndlEchpPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining retriever function\n",
        "retriever = vectorstore.as_retriever(search_type='similarity',search_kwargs={'k': 3}\n",
        ")\n"
      ],
      "metadata": {
        "id": "pTrxt0vcg0aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The k value is 3 helps in retieving top 3 most similiar document."
      ],
      "metadata": {
        "id": "HEV2XFvFsaK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#checking retriever\n",
        "rel_docs = retriever.get_relevant_documents(\"Hair loss\")\n",
        "rel_docs"
      ],
      "metadata": {
        "id": "-BuSPpVY_OtS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjNHEvmKLjCk"
      },
      "source": [
        "Defining the Response Generator\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbnfuixLMfYk"
      },
      "outputs": [],
      "source": [
        "qna_system_message = \"\"\"\n",
        "\"You are a medical information expert. Answer strictly based on the content of the provided medical manual.Do not use any external sources.\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Just answer the question directly, as if you are the expert providing the answer yourself.\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.If the answer is not found in the manual, respond \"I don't know\".\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "qna_user_message_template = \"\"\"\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{question}\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-5jnLIvoR9my"
      },
      "outputs": [],
      "source": [
        "# Response Function\n",
        "def generate_rag_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=k)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "\n",
        "    # Combine document chunks into a single context\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    user_message = qna_user_message_template.replace('{context}', context_for_query)\n",
        "    user_message = user_message.replace('{question}', user_input)\n",
        "\n",
        "    prompt = qna_system_message + '\\n' + user_message\n",
        "\n",
        "    # Generate the response\n",
        "    try:\n",
        "        response = llm(\n",
        "                  prompt=prompt,\n",
        "                  max_tokens=max_tokens,\n",
        "                  temperature=temperature,\n",
        "                  top_p=top_p,\n",
        "                  top_k=top_k\n",
        "                  )\n",
        "\n",
        "        # Extract and print the model's response\n",
        "        response = response['choices'][0]['text'].strip()\n",
        "    except Exception as e:\n",
        "        response = f'Sorry, I encountered the following error: \\n {e}'\n",
        "\n",
        "    return response"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r-QJsvmSNdSQ"
      },
      "source": [
        "Question Answering using RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vsAh613iOqOK"
      },
      "outputs": [],
      "source": [
        "#Question Answering using RAG\n",
        "#Query 1: What is the protocol for managing sepsis in a critical care unit?\n",
        "user_input=\"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "print(generate_rag_response(user_input))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 2:What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n",
        "user_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
        "print(generate_rag_response(user_input))"
      ],
      "metadata": {
        "id": "N-j5lAhK-N5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n",
        "user_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
        "print(generate_rag_response(user_input))\n"
      ],
      "metadata": {
        "id": "WpUalET6-N15"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n",
        "user_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
        "print(generate_rag_response(user_input))"
      ],
      "metadata": {
        "id": "Hsgn2XQohGaj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n",
        "user_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "print(generate_rag_response(user_input))"
      ],
      "metadata": {
        "id": "8U7QOnvg-Nq7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observations for RAG answers:\n",
        "\n",
        "All the answers are clinically relevant. The k=3 retrieves top 3 documents. The context is concise and relevant.The k value can be increase to get more context. The max tokens=128 provides concise answer can be altered to get detailed response. The temperature=0 gives focused answer it can be increased to get creative answer.The top p value can be reduced to get focused answer. No hallucination or repetition of phrases is seen."
      ],
      "metadata": {
        "id": "vH7ZGkXnR4_a"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7QXwq4u3OrNX"
      },
      "source": [
        "#Fine-tuning Parameters\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section we will be fine tuning retriver and llm parameter by using 5 combination.\n",
        "k is number of top documents retrieved.The higher k value more content retrieved but may increase noise and lower k value less content gives more focused but may miss useful information.The search_type is the method used to retrieve results.Similarity: Retrieves based on cosine similarity helps in best matching chunks and MMR (Maximal Marginal Relevance): It balances similarity and diversity of results reduces duplication. fetch_k is: Number of documents initially fetched before re-ranking. Higher fetch_k values increases pool for better ranking and lower fetch_k values Faster, can miss better matches.The lambda_mult is balancing factor between relevance and diversity (0 to 1).\n",
        "Higher values 1.0 helps in prioritizes similarity more and lower values helps prioritizes diversity more."
      ],
      "metadata": {
        "id": "QuJG3CeTwJLp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combination 1\n",
        "\n",
        "# System message for the LLM to strictly follow\n",
        "qna_system_message = \"\"\"\n",
        "You are a medical information expert. Answer strictly based on the content of the provided medical manual. Do not use any external sources. If the information is not present in the manual, respond with I don’t know.\n",
        "\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "# Define the 5 medical questions\n",
        "queries = [\n",
        "    \"Q1) What is the protocol for managing sepsis in a critical care unit?\",\n",
        "    \"Q2) What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Q3) What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"Q4) What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"Q5) What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "]\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 2}\n",
        ")\n",
        "\n",
        "\n",
        "# QA prompt format\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"{qna_system_message}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Run all queries\n",
        "for query in queries:\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = format_prompt(context, query)\n",
        "\n",
        "    answer = llm(prompt, temperature=0, max_tokens=300, top_p=0)\n",
        "\n",
        "    print(f\"\\n Query: {query}\")\n",
        "    print(f\" Answer: {answer}\")"
      ],
      "metadata": {
        "id": "qvFJAY5N3Rtb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is simple combination with k = 2 helps in retrieval top two similiar document.  This value is very low the top 2 will be insufficient for detailed answers it can lead to missing important information. The temperature and top p value is 0 and max token is less so a focused and concise response is seen."
      ],
      "metadata": {
        "id": "fykQ8_tDgdg8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Combination 2\n",
        "\n",
        "\n",
        "# System message for the LLM to strictly follow\n",
        "qna_system_message = \"\"\"\n",
        "You are a medical information expert. Answer strictly based on the content of the provided medical manual. Do not use any external sources. If the information is not present in the manual, respond with I don’t know.\n",
        "\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "This context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "# Define the 5 medical questions\n",
        "queries = [\n",
        "    \"Q1) What is the protocol for managing sepsis in a critical care unit?\",\n",
        "    \"Q2) What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Q3) What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"Q4) What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"Q5) What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "]\n",
        "\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 6}\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# QA prompt format\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"{qna_system_message}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Run all queries\n",
        "for query in queries:\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = format_prompt(context, query)\n",
        "\n",
        "    answer = llm(prompt, temperature=0, max_tokens=500, top_p=0)\n",
        "\n",
        "    print(f\"\\n Query: {query}\")\n",
        "    print(f\" Answer:\\n{answer}\")"
      ],
      "metadata": {
        "id": "tsQPtOaFj2r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this combination k is 6 so top 6 semantically similiar documents are retrieved. It retrieves more documents for comprehensive clinical questions but may include some irrelevant info.The temperature and p value is 0 to keep answer focused and concise.Here max token higher to get little more detailed answer.It retrieves more documents for comprehensive clinical questions but may include some irrelevant info.The temperature and p value is 0 to keep answer focused and concise."
      ],
      "metadata": {
        "id": "hx5C773AiAbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combination 3\n",
        "\n",
        "\n",
        "# System message for the LLM to strictly follow\n",
        "qna_system_message = \"\"\"\n",
        "You are a medical information expert. Answer strictly based on the content of the provided medical manual. Do not use any external sources. If the information is not present in the manual, respond with I don’t know.\n",
        "\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "# Define the 5 medical questions\n",
        "queries = [\n",
        "    \"Q1) What is the protocol for managing sepsis in a critical care unit?\",\n",
        "    \"Q2) What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Q3) What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"Q4) What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"Q5) What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "]\n",
        "\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={\"k\": 4, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
        ")\n",
        "\n",
        "# QA prompt format\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"{qna_system_message}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Run all queries\n",
        "for query in queries:\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = format_prompt(context, query)\n",
        "\n",
        "    answer = llm(prompt, temperature=0.5, max_tokens=600, top_p=0.6)\n",
        "\n",
        "    print(f\"\\n Query: {query}\")\n",
        "    print(f\" Answer:\\n{answer}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "jTB3EQtdj8DU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this combination the value of k is minimal neither too high or low k=4, fetch k is 10, lambda mult is 0.5. MMR helps to retrieve diverse yet relevant information. The top p and temperature is also balanced. This allows for controlled variablity. Higher fetch k is 8 to 12 is helpful ensures to avoid missing relevant but rare information. Balanced lambda mult 0.5 is good for factual answers but avoids repeating the same points. The top p value and temperature value is balanced to get controlled yet varied response."
      ],
      "metadata": {
        "id": "tuzLG2Zci38n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#combination 4\n",
        "\n",
        "# System message for the LLM to strictly follow\n",
        "qna_system_message = \"\"\"\n",
        "You are a medical information expert. Answer strictly based on the content of the provided medical manual. Do not use any external sources. If the information is not present in the manual, respond with I don’t know.\n",
        "\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "# Define the 5 medical questions\n",
        "queries = [\n",
        "    \"Q1) What is the protocol for managing sepsis in a critical care unit?\",\n",
        "    \"Q2) What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Q3) What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"Q4) What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"Q5) What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "]\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='similarity',\n",
        "    search_kwargs={'k': 3}\n",
        ")\n",
        "\n",
        "\n",
        "# QA prompt format\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"{qna_system_message}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Run all queries\n",
        "for query in queries:\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = format_prompt(context, query)\n",
        "\n",
        "    answer = llm(prompt, temperature=0.2, max_tokens=500, top_p=0.2)\n",
        "\n",
        "    print(f\"\\n Query: {query}\")\n",
        "    print(f\" Answer:\\n{answer}\")"
      ],
      "metadata": {
        "id": "qAnYEfgPj-V2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This combination has minimal value of k it ensures retrieval of fewer documents. The value of k is low can cause missing out important information. Here the temperature and p value is slightly increased. This combination focuses on generating factual answers by reducing randomness."
      ],
      "metadata": {
        "id": "KcZcS7thkHUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combination 5\n",
        "\n",
        "# System message for the LLM to strictly follow\n",
        "qna_system_message = \"\"\"\n",
        "You are a medical information expert. Answer strictly based on the content of the provided medical manual. Do not use any external sources. If the information is not present in the manual, respond with I don’t know.\n",
        "\n",
        "User input will have the context required by you to answer user questions.\n",
        "This context will begin with the token: ###Context.\n",
        "The context contains references to specific portions of a document relevant to the user query.\n",
        "\n",
        "User questions will begin with the token: ###Question.\n",
        "\n",
        "Please answer only using the context provided in the input. Do not mention anything about the context in your final answer.\n",
        "\n",
        "If the answer is not found in the manual, respond \"I don't know\".\n",
        "\"\"\"\n",
        "\n",
        "# Define the 5 medical questions\n",
        "queries = [\n",
        "    \"Q1) What is the protocol for managing sepsis in a critical care unit?\",\n",
        "    \"Q2) What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\",\n",
        "    \"Q3) What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\",\n",
        "    \"Q4) What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\",\n",
        "    \"Q5) What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "]\n",
        "\n",
        "# Setup retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type='mmr',\n",
        "    search_kwargs={'k': 5,\"fetch_k\": 8, \"lambda_mult\": 0.8}\n",
        ")\n",
        "\n",
        "\n",
        "# QA prompt format\n",
        "def format_prompt(context, query):\n",
        "    return f\"\"\"{qna_system_message}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Question\n",
        "{query}\n",
        "\"\"\"\n",
        "\n",
        "# Run all queries\n",
        "for query in queries:\n",
        "    docs = retriever.get_relevant_documents(query)\n",
        "    context = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    prompt = format_prompt(context, query)\n",
        "\n",
        "    answer = llm(prompt, temperature=0.8, max_tokens=300, top_p=0.9)\n",
        "\n",
        "\n",
        "    print(f\"\\n Query: {query}\")\n",
        "    print(f\" Answer:\\n{answer}\")"
      ],
      "metadata": {
        "id": "PyYQvnxrkBIv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Combination 5 High Diversity MMR λ=0.8, k=5, fetch k = 8\n",
        "It allows diversity in retrieved chunks. The temperature and top p value is high which causes answer to be deviate from factual information. It increases randomness.This combination has possiblity of hallucination in response.This combination is less reliable for medical use case.\n"
      ],
      "metadata": {
        "id": "4gyPiVwRk6RC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Conclusion:\n",
        "\n",
        "Best combination:\n",
        "\n",
        "MMR with a balanced lambda mult (0.5) effectively retrieves a good mix of relevant and diverse information from the manual, which is crucial for providing comprehensive medical answers.\n",
        "The k=4 and fetch k=10 settings likely provide enough context to generate detailed responses without overwhelming the model.\n",
        "The moderate temperature and top_p values strike a balance between generating focused, factual answers and allowing for some flexibility in phrasing, while minimizing the risk of hallucination.\n",
        "We observed, this combination produced clinically relevant answers devoid of significant hallucinations or repetitions.\n",
        "The combination 4 low temperature for strict factual answers and combination 3 seems to offer the best overall performance for a medical RAG system where both comprehensiveness and accuracy are important.\n",
        "\n",
        "It is generally better to avoid very high temperatures and low k values to ensure reliability and accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "gmIPbn1WKrqV"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61J81U7fO4v5"
      },
      "source": [
        "# Output Evaluation\n",
        "\n",
        "We will be using the LLM-as-a-judge method to check the quality of the RAG system on two parameters - retrieval and generation. We illustrate this evaluation based on the answeres generated to the question from the previous section.\n",
        "The same Mistral model for evaluation, so basically here the llm is rating itself on how well he has performed in the task."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Defining the Evaluation Prompts\n",
        "groundedness_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "The answer should be derived only from the information presented in the context\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the answer as per the metric in 2 lines.\n",
        "2. Next, evaluate the extent to which the metric is followed.\n",
        "3. Use the previous information to rate the answer using the evaluaton criteria and assign a score.\n",
        "\n",
        "Example Output:\n",
        "Groundedness:\n",
        "Steps\n",
        "Rating:\n",
        "Explanation: The answer is directly supported by ......\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "cIqBUzPy6pfX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "relevance_rater_system_message = \"\"\"\n",
        "You are tasked with rating AI generated answers to questions posed by users.\n",
        "You will be presented a question, context used by the AI system to generate the answer and an AI generated answer to the question.\n",
        "In the input, the question will begin with ###Question, the context will begin with ###Context while the AI generated answer will begin with ###Answer.\n",
        "\n",
        "Evaluation criteria:\n",
        "The task is to judge the extent to which the metric is followed by the answer.\n",
        "1 - The metric is not followed at all\n",
        "2 - The metric is followed only to a limited extent\n",
        "3 - The metric is followed to a good extent\n",
        "4 - The metric is followed mostly\n",
        "5 - The metric is followed completely\n",
        "\n",
        "Metric:\n",
        "Relevance measures how well the answer addresses the main aspects of the question, based on the context.\n",
        "Consider whether all and only the important aspects are contained in the answer when evaluating relevance.\n",
        "\n",
        "Instructions:\n",
        "1. First write down the steps that are needed to evaluate the context as per the metric in 2 lines.\n",
        "2. Next, evaluate the extent to which the metric is followed.\n",
        "3. Use the previous information to rate the context using the evaluaton criteria and assign a score.\n",
        "\n",
        "Example Output:\n",
        "Relevance:\n",
        "Steps\n",
        "Rating:\n",
        "Explanation: The answer is directly supported by .......\n",
        "\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "sO3oJOym61z0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_message_template = \"\"\"\n",
        "###Question\n",
        "{question}\n",
        "\n",
        "###Context\n",
        "{context}\n",
        "\n",
        "###Answer\n",
        "{answer}\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "VCUDwdE26tYZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6NoPt-VWGX-"
      },
      "outputs": [],
      "source": [
        "#Response function\n",
        "def generate_ground_relevance_response(user_input,k=3,max_tokens=128,temperature=0,top_p=0.95,top_k=50):\n",
        "    global qna_system_message,qna_user_message_template\n",
        "    # Retrieve relevant document chunks\n",
        "    relevant_document_chunks = retriever.get_relevant_documents(query=user_input,k=3)\n",
        "    context_list = [d.page_content for d in relevant_document_chunks]\n",
        "    context_for_query = \". \".join(context_list)\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    prompt = f\"\"\"[INST]{qna_system_message}\\n\n",
        "                {'user'}: {qna_user_message_template.format(context=context_for_query, question=user_input)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    response = llm(\n",
        "            prompt=prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            )\n",
        "\n",
        "    answer =  response[\"choices\"][0][\"text\"]\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    groundedness_prompt = f\"\"\"[INST]{groundedness_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    # Combine user_prompt and system_message to create the prompt\n",
        "    relevance_prompt = f\"\"\"[INST]{relevance_rater_system_message}\\n\n",
        "                {'user'}: {user_message_template.format(context=context_for_query, question=user_input, answer=answer)}\n",
        "                [/INST]\"\"\"\n",
        "\n",
        "    response_1 = llm(\n",
        "            prompt=groundedness_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            )\n",
        "\n",
        "    response_2 = llm(\n",
        "            prompt=relevance_prompt,\n",
        "            max_tokens=max_tokens,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            top_k=top_k,\n",
        "            stop=['INST'],\n",
        "            )\n",
        "\n",
        "    return response_1['choices'][0]['text'],response_2['choices'][0]['text']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwitPCV_Pz-u"
      },
      "outputs": [],
      "source": [
        "#Query 1:  What is the protocol for managing sepsis in a critical care unit?\n",
        "user_input = \"What is the protocol for managing sepsis in a critical care unit?\"\n",
        "ground,rel = generate_ground_relevance_response(user_input)\n",
        "print(ground)\n",
        "print(rel)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "For the answer 1 llm gave a groundedness rating as 5, explaining that the answer was directly derived from the context, specifically mentioning the fluid resuscitation details.The llm gave a relevance rating as 5, stating the context was highly relevant and provided a detailed description of the protocol."
      ],
      "metadata": {
        "id": "k02AklyLvgbD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 2:What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\n",
        "user_input = \"What are the common symptoms for appendicitis, and can it be cured via medicine? If not, what surgical procedure should be followed to treat it?\"\n",
        "ground,rel = generate_ground_relevance_response(user_input)\n",
        "print(ground)\n",
        "print(rel)"
      ],
      "metadata": {
        "id": "bMWw7iCgxxzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "For the answer 2 llm gave a groundedness rating as 5, as it identifies the symptoms of appendicitis and states that surgery is the standard treatment.The llm gave a relevance rating as 4, provided a description of the symptoms and treatment options but did not explicitly state why one mark was deducted."
      ],
      "metadata": {
        "id": "Nrj8T8VUvXce"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 3: What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\n",
        "user_input = \"What are the effective treatments or solutions for addressing sudden patchy hair loss, commonly seen as localized bald spots on the scalp, and what could be the possible causes behind it?\"\n",
        "ground,rel = generate_ground_relevance_response(user_input)\n",
        "print(ground)\n",
        "print(rel)"
      ],
      "metadata": {
        "id": "NlI03fsmF2dU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "For the answer 3 llm gave groundedness rating as 4 and relevance 5 as it covers detailed information of Alopecia Areata.It does not mention why 1 mark was deducted may be some minor details of the answer was not fully supported by provided context chunks."
      ],
      "metadata": {
        "id": "ibO8xgwn3Map"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 4: What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\n",
        "user_input = \"What treatments are recommended for a person who has sustained a physical injury to brain tissue, resulting in temporary or permanent impairment of brain function?\"\n",
        "ground,rel = generate_ground_relevance_response(user_input)\n",
        "print(ground)\n",
        "print(rel)"
      ],
      "metadata": {
        "id": "vycfIsKSF2QM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "For answer 4 the llm rates groundedness 3 as information about spinal cord injuries and immobilization, which is not directly related to the question or context. The relevance rating is 3 suggest answer might not have fully addressed all parts of the question based on the available context or  included some irrelevant information that has reduced its relevance score."
      ],
      "metadata": {
        "id": "gc8BeVRhzRzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Query 5: What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\n",
        "user_input = \"What are the necessary precautions and treatment steps for a person who has fractured their leg during a hiking trip, and what should be considered for their care and recovery?\"\n",
        "ground,rel = generate_ground_relevance_response(user_input)\n",
        "print(ground)\n",
        "print(rel)"
      ],
      "metadata": {
        "id": "jlnh_pVlF34U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation\n",
        "\n",
        "For the answer 5 llm gives groundedness rating 4 it mentions the answer misess application of ice and compression in first 24-48 hrs. It can be llm groundedness rating  for this specific point was incorrect as RAG answer mentions application of ice and compression first 24 to 48 hours. The llm rates relevance as 5 as answer addresses all important aspects of question."
      ],
      "metadata": {
        "id": "f9LKmh1eu8uE"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ep-Aj7WUXwKY"
      },
      "source": [
        "# Actionable Insights and Business Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observation:\n",
        "\n",
        "First we used questions to get raw llm answer. Next by altering prompt and llm parameters the performance was monitored.There was good improvement in performance. Then we answered the question based on RAG. Next we did fine tuning retreiver and llm parameters to see the performance. Further we evaluated the RAG response by using llm as judge. The answers were given considerably good rating.\n",
        "\n",
        "Here best combination is combination 3 MMR with balanced parameter we can deploy model for medical use case.\n",
        "\n",
        "MMR (Maximal Marginal Relevance) helps minimize redundancy and ensures retrieval of diverse but relevant chunks of context.\n",
        "Balanced lambda mult 0.5 finds a balanced between relevance and diversity.\n",
        "Higher fetch k and k helps in  retrieval of information retrieved for detailed questions.\n",
        "Moderate temperature keeps answers focused and reduces noise.\n",
        "The top p ensures more controlled variablity and max tokens to get detailed response important for medical questions.\n",
        "\n",
        "\n",
        "Actionable insights:\n",
        "\n",
        "Retrieval parameter k is critical to retrieve more documents.\n",
        "\n",
        "The chunk overlap ensures coherence.\n",
        "\n",
        "The max tokens higher values yield detailed responses, while simple queries result in concise outputs despite large token limits due to prompt design and zero temperature.\n",
        "\n",
        "We can tailor prompts to get nuanced response.\n",
        "\n",
        "The temperature and top p value settings can be altered to control response length and creativity.\n",
        "\n",
        "We can continuously adjust RAG parameters based on specific use cases for optimal performance.\n",
        "\n",
        "We can further tune groundedness and relevance prompts in evaluations to ensure reliable and contextually accurate outputs.\n",
        "\n",
        "We can further fine-tune parameters to get creative and relevant response.\n",
        "\n",
        "\n",
        "Recommendations:\n",
        "\n",
        "In medical use cases it is better to avoid\n",
        "-High temperatures 0.9 can hallucinate\n",
        "-Low k may miss important context\n",
        "-Low max_tokens risks truncating complex medical explanations\n",
        "\n",
        "We can include domain expert medical practitioner to validate responses.\n",
        "\n",
        "It is better to include a disclaimer that it is not a substitute for professional medical advice.\n"
      ],
      "metadata": {
        "id": "PCy70AZiGeuO"
      }
    }
  ]
}